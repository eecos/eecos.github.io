{"meta":{"title":"lijiating’s Blog","subtitle":"凡是挣扎过来的人都是真金不怕火炼的；任何幻灭都不能动摇他们的信仰：因为他们一开始就知道信仰之路和幸福之路全然不同，而他们是不能选选择的，只有往这条路走，别的都是死路。这样的自信不是一朝一夕所能养成的。你绝不能以此期待那些十五岁左右的孩子。在得到这个信念之之前，先得受尽悲痛，流尽眼泪。可是这样是好的，应该要这样……","description":"lijiating’s Blog","author":"李加廷","url":"https://mrjerryli.github.io"},"pages":[{"title":"about","date":"2018-10-30T02:34:36.000Z","updated":"2019-08-11T01:39:47.196Z","comments":true,"path":"about/index.html","permalink":"https://mrjerryli.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-10-30T02:33:58.000Z","updated":"2019-08-11T03:37:23.154Z","comments":true,"path":"categories/index.html","permalink":"https://mrjerryli.github.io/categories/index.html","excerpt":"","text":""},{"title":"photo","date":"2018-10-30T02:33:58.000Z","updated":"2019-08-11T01:39:47.244Z","comments":true,"path":"photo/index.html","permalink":"https://mrjerryli.github.io/photo/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-10-30T02:33:58.000Z","updated":"2019-08-11T01:39:47.244Z","comments":true,"path":"tags/index.html","permalink":"https://mrjerryli.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"jira与confluence的简介","slug":"jira与confluence的简介","date":"2020-05-27T02:51:16.000Z","updated":"2020-05-27T02:54:34.150Z","comments":true,"path":"post/8e32da8b.html","link":"","permalink":"https://mrjerryli.github.io/post/8e32da8b.html","excerpt":"","text":"Confluence和jira的使用JIRA是Atlassian公司出品的项目与事务管理跟踪工具，被广泛应用于需求收集、流程审批、任务跟踪、项目跟踪、缺陷跟踪、客户服务、和敏捷管理等工作领域。 Confluence（也就是 wiki ）的使用几乎贯穿了整个敏捷过程，如：在产品设计时编写产品需求，在会议讨论时编写会议笔记，在冲刺结束后编写冲刺回顾……Confluence自身也为这些需求提供了丰富的文档模板。 画了两张粗糙的思维导图，加强理解 img jira思维导图 img wiki思维导图 先预留一个位置，之后还会慢慢补充~~ 附： 敏捷开发中的一些概念 scrum team: 整个组织架构中可进行独立开发的最小团队, 一般人数控制在5~10人左右 sprint：项目开发过程中最小迭代周期，根据同的项目周期不同；现有产品维护1~5天，二次开发5~10，新项目5~30，业务复杂或开发所用语言较多或开发复杂度较高10~45 point：不可拆分的最小功能点数，1point等于多少小时，可以由项目主管按照项目的实际情况指定。 Product Backlog：由PO负责管理，将所有epic按优先顺序排列的一个产品需求列表。 Sprint Backlog：通过Sprint Planning Meeting，由PM、PO、SM从product backlog中挑选出一个或多个Story作为本次迭代完成的目标 epic：一个包含完成功能的需求描述，可以被拆分为1个或多个stroy，一般有PO划分 story：一个不可拆分子功能描述，可以被拆分为1个过多个task，一般由PO将epic拆分为story，在Sprint Planning Pre-Plan Meeting中PO、SM确定story是否合理并做相应的调整。 task：最小可用于开发的任务，在sprint开始时由ST将story拆分为task，并在当前sprint中完成 product Master(PM)：管理整个项目的整体进度，也可叫做scrum master of scrums. product owner(PO)：管理整个项目或多个scrum team的需求，并将需求转换为epic或story，指定软件交付日期，指定交付标准，有权力接受或拒绝开发团队的工作成果 scrum master(SM)：整体控制一个scrum能过承接的point数；监控每个sprint的整体进度；协调与其他scrum team之间的沟通 technique leader(TL)：整个项目或多个scrum team或一个scrum team的技术专家，主要负责在开发过程中给出技术解决方案或建议 scrum team member(ST)：scrum项目成员，主要负责开发，以及将指派给自己的story拆分为task Sprint Planning Pre-Plan Meeting：PO、SM确定挑选story为后期sprint做准备，一般提前2个sprint Sprint Planning Meeting：在sprint之前PO将下个sprint需要完成的story交付给SM，SM需要根据目前ST的能力确认point数，此时应该确认或拒绝下一个sprint所有需求。 Sprint start meeting：在sprint的第一天将当前sprint的story指派给相关ST，除非TL提出异议并且征得PO或PM同意，否则所有stroy必须在当前sprint完成。 Srpint Review Meeting：也叫Srpint Demo Meeting，将这个sprint的成果演示给PO，由PO确认演示内容是否符合交付标准，并提出改善意见。 Sprint Retrospective Meeting：回顾会议，SM、ST、TL，PO参与对于上一个sprint成果总结，优点继续保持，缺点或缺陷根据实际情况由PO决定是否建立story放入Product Backlog中，在以后或下一个sprint中完善。 Daily Scrum Meeting：也叫stand up meeting，每天占用5~15分钟汇报、分享、提出问题。特别注意此会议不解决任何问题。 Scrum of Scurms meeting：大型项目需要，多个scrum team协同开发时，多个Team SM参与讨论会议；对上一个sprint总结汇报，并提出相关意见，如果涉及到代码改善意见，需要请PO确认是否建立story；下一个sprint资源确认，是否需要其他Team协助，是否有人力资源调整等问题。","categories":[{"name":"Test","slug":"Test","permalink":"https://mrjerryli.github.io/categories/Test/"}],"tags":[{"name":"atlassian","slug":"atlassian","permalink":"https://mrjerryli.github.io/tags/atlassian/"}],"keywords":[{"name":"Test","slug":"Test","permalink":"https://mrjerryli.github.io/categories/Test/"}]},{"title":"docker搭建redis集群","slug":"docker搭建redis集群","date":"2020-05-19T03:46:20.000Z","updated":"2020-05-19T03:50:27.627Z","comments":true,"path":"post/1672ce0a.html","link":"","permalink":"https://mrjerryli.github.io/post/1672ce0a.html","excerpt":"","text":"Docker-compose部署redis-cluster集群参考链接：https://www.jianshu.com/p/b7dea62bcd8b 参考镜像：https://github.com/publicisworldwide/docker-stacks/tree/master/oracle-linux/environments/storage/redis-cluster 1.创建数据目录 123456root@rede:# pwd/data/redisroot@rede:# mkdir 700&#123;1..6&#125; ##这个目录和下边yml文件的volumes对应root@rede:# mkdir 700&#123;1..6&#125;/data root@rede:# ls 7001 7002 7003 7004 7005 7006 docker-compose.yml 2.创建一个docker-compose.yml文件，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556version: '3'services: redis1: image: publicisworldwide/redis-cluster network_mode: host restart: always volumes: - /data/redis/7001/data:/data environment: - REDIS_PORT=7001 redis2: image: publicisworldwide/redis-cluster network_mode: host restart: always volumes: - /data/redis/7002/data:/data environment: - REDIS_PORT=7002 redis3: image: publicisworldwide/redis-cluster network_mode: host restart: always volumes: - /data/redis/7003/data:/data environment: - REDIS_PORT=7003 redis4: image: publicisworldwide/redis-cluster network_mode: host restart: always volumes: - /data/redis/7004/data:/data environment: - REDIS_PORT=7004 redis5: image: publicisworldwide/redis-cluster network_mode: host restart: always volumes: - /data/redis/7005/data:/data environment: - REDIS_PORT=7005 redis6: image: publicisworldwide/redis-cluster network_mode: host restart: always volumes: - /data/redis/7006/data:/data environment: - REDIS_PORT=7006 这里用的别人一个现成的镜像 publicisworldwide/redis-cluster ，自己制作也可以，后续会发布制作过程，不过不建议自己做，有什么需要更改的可以在现有的镜像基础上进行更改之后commit。镜像可以自己下载，也可以等启动服务的时候他自动下载。 这里用的是host网络模式，redis挂载到之前创建的目录下。 如果不想使用主机模式可以文章开头的链接，具体我这里就不写了，自己看。至于docker-compose的安装网上一大堆文章，这里不赘述。 3.创建文件之后，启动服务 12345678910111213141516root@rede: docker-compose up -d Creating redis_redis1_1 ... doneCreating redis_redis5_1 ... doneCreating redis_redis4_1 ... doneCreating redis_redis3_1 ... doneCreating redis_redis6_1 ... doneCreating redis_redis2_1 ... done root@rede: docker-compose ps Name Command State Ports---------------------------------------------------------------redis_redis1_1 /usr/local/bin/entrypoint. ... Up redis_redis2_1 /usr/local/bin/entrypoint. ... Up redis_redis3_1 /usr/local/bin/entrypoint. ... Up redis_redis4_1 /usr/local/bin/entrypoint. ... Up redis_redis5_1 /usr/local/bin/entrypoint. ... Up redis_redis6_1 /usr/local/bin/entrypoint. ... Up 状态都为Up，说明服务均正常启动，如果有状态不正常的容器，看看自己服务器的端口是不是被占用了。 4.redis-cluster集群配置 上述只是启动了6个redis容器，并没有设置集群，随便进入一个redis容器，执行下面的命令，IP替换成自己宿主机的IP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566root@rede:/data/redis ▶ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa0c7047c2a0e publicisworldwide/redis-cluster &quot;/usr/local/bin/entr…&quot; 2 minutes ago Up 2 minutes redis_redis3_1f165a7c962ca publicisworldwide/redis-cluster &quot;/usr/local/bin/entr…&quot; 2 minutes ago Up 2 minutes redis_redis1_18ce8ec23974e publicisworldwide/redis-cluster &quot;/usr/local/bin/entr…&quot; 2 minutes ago Up 2 minutes redis_redis4_11c37f7b71a21 publicisworldwide/redis-cluster &quot;/usr/local/bin/entr…&quot; 2 minutes ago Up 2 minutes redis_redis6_1810b0ff90710 publicisworldwide/redis-cluster &quot;/usr/local/bin/entr…&quot; 2 minutes ago Up 2 minutes redis_redis2_1e5a38e1cc280 publicisworldwide/redis-cluster &quot;/usr/local/bin/entr…&quot; 2 minutes ago Up 2 minutes redis_redis5_1df94ddfb5d1f zookeeper:3.4.11 &quot;/docker-entrypoint.…&quot; 20 hours ago Up 20 hours 2888/tcp, 3888/tcp, 0.0.0.0:2182-&gt;2181/tcp zookeeper_25eb43213b27a zookeeper:3.4.11 &quot;/docker-entrypoint.…&quot; 20 hours ago Up 20 hours 2888/tcp, 3888/tcp, 0.0.0.0:2183-&gt;2181/tcp zookeeper_3d17a6180907e zookeeper:3.4.11 &quot;/docker-entrypoint.…&quot; 20 hours ago Up 20 hours 2888/tcp, 0.0.0.0:2181-&gt;2181/tcp, 3888/tcp zookeeper_1root@rede:# docker exec -ti d3a904cc0d5a bash root@rede:/data# redis-cli --cluster create 192.168.1.100:7001 192.168.1.100:7002 192.168.1.100:7003 192.168.1.100:7004 192.168.1.100:7005 192.168.1.100:7006 --cluster-replicas 1&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 192.168.1.100:7004 to 192.168.1.100:7001Adding replica 192.168.1.100:7005 to 192.168.1.100:7002Adding replica 192.168.1.100:7006 to 192.168.1.100:7003&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity[WARNING] Some slaves are in the same host as their masterM: 7237efa105ff4246c02da08c0e65c18246755f2f 192.168.1.100:7001 slots:[0-5460] (5461 slots) masterM: 241d6eb2abcd9a6d41db5231fcd80fac1d452bee 192.168.1.100:7002 slots:[5461-10922] (5462 slots) masterM: 493083f2733bc181295f5b9a9fa9bc6fdf821958 192.168.1.100:7003 slots:[10923-16383] (5461 slots) masterS: b60ef6cbea0ae06566d17db4a1b4452e56543419 192.168.1.100:7004 replicates 241d6eb2abcd9a6d41db5231fcd80fac1d452beeS: b48e546e963f694eac91d49b285040bb2bec5513 192.168.1.100:7005 replicates 493083f2733bc181295f5b9a9fa9bc6fdf821958S: 259de2d6be917e64cdc01604521da45cc1f8d842 192.168.1.100:7006 replicates 7237efa105ff4246c02da08c0e65c18246755f2fCan I set the above configuration? (type &apos;yes&apos; to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join....&gt;&gt;&gt; Performing Cluster Check (using node 192.168.1.100:7001)M: 7237efa105ff4246c02da08c0e65c18246755f2f 192.168.1.100:7001 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: 241d6eb2abcd9a6d41db5231fcd80fac1d452bee 192.168.1.100:7002 slots:[5461-10922] (5462 slots) master 1 additional replica(s)M: 493083f2733bc181295f5b9a9fa9bc6fdf821958 192.168.1.100:7003 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: b60ef6cbea0ae06566d17db4a1b4452e56543419 192.168.1.100:7004 slots: (0 slots) slave replicates 241d6eb2abcd9a6d41db5231fcd80fac1d452beeS: 259de2d6be917e64cdc01604521da45cc1f8d842 192.168.1.100:7006 slots: (0 slots) slave replicates 7237efa105ff4246c02da08c0e65c18246755f2fS: b48e546e963f694eac91d49b285040bb2bec5513 192.168.1.100:7005 slots: (0 slots) slave replicates 493083f2733bc181295f5b9a9fa9bc6fdf821958[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 至此，基于docker部署的redis-cluster集群就已经完成","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"},{"name":"Docker","slug":"Server/Docker","permalink":"https://mrjerryli.github.io/categories/Server/Docker/"}],"tags":[{"name":"centos","slug":"centos","permalink":"https://mrjerryli.github.io/tags/centos/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"},{"name":"Docker","slug":"Server/Docker","permalink":"https://mrjerryli.github.io/categories/Server/Docker/"}]},{"title":"DockerCompose搭zookeeper集群","slug":"容器化部署zookeeper","date":"2020-05-17T18:24:08.000Z","updated":"2020-05-17T19:02:29.507Z","comments":true,"path":"post/7b97a5e1.html","link":"","permalink":"https://mrjerryli.github.io/post/7b97a5e1.html","excerpt":"","text":"准备zookeeper镜像 这里使用官方提供的镜像，版本为3.4.11 docker-compose分布式管理组件，具体安装以及使用方法见github项目 准备docker-compose.yml文件 这里要注意下yml语法，key冒号后面有空格 这里采用官方镜像提供默认配置，可根据docker-compose配置说明做调整 12345678910111213141516171819202122232425262728293031323334353637383940414243444546version: &apos;3.1&apos;services: zoo1: image: zookeeper:3.4.11 restart: always hostname: zoo1 container_name: zookeeper_1 #domainname: ports: - 2181:2181 volumes: - /usr/local/docker_app/zookeeper/zoo1/data:/data - /usr/local/docker_app/zookeeper/zoo1/datalog:/datalog environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo2: image: zookeeper:3.4.11 restart: always hostname: zoo2 container_name: zookeeper_2 ports: - 2182:2181 volumes: - /usr/local/docker_app/zookeeper/zoo2/data:/data - /usr/local/docker_app/zookeeper/zoo2/datalog:/datalog environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 zoo3: image: zookeeper:3.4.11 restart: always hostname: zoo3 container_name: zookeeper_3 ports: - 2183:2181 volumes: - /usr/local/docker_app/zookeeper/zoo3/data:/data - /usr/local/docker_app/zookeeper/zoo3/datalog:/datalog environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888 docker-compse命令提供了一系列子命令 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Define and run multi-container applications with Docker.Usage: docker-compose [-f &lt;arg&gt;...] [options] [COMMAND] [ARGS...] docker-compose -h|--helpOptions: -f, --file FILE Specify an alternate compose file (default: docker-compose.yml) -p, --project-name NAME Specify an alternate project name (default: directory name) --verbose Show more output --no-ansi Do not print ANSI control characters -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don't check the daemon's hostname against the name specified in the client certificate (for example if your docker host is an IP address) --project-directory PATH Specify an alternate working directory (default: the path of the Compose file)Commands: build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information 校验指定配置文件正确性，默认配置文件名称docker-compose.yml或docker-compose.yml，我这里使用自定义名称zookeeper-compose.yml 注意：如若需指定配置文件，必须在docker-compose后面指定，不能在config等子命令后面指定，否则无效；如果你使用默认配置文件名称，不需要显式指定-f docker-compose.yml 1234// 查看config命令参数$ docker-compose -f zookeeper-compose.yml config --help// 校验配置文件，不打印$ docker-compose -f zookeeper-compose.yml config -q 启动zookeeper集群12// -d 后台启动$ docker-compose -f zookeeper-compose.yml up -d 查看容器启动情况123456$ docker-compose -f zookeeper-compose.yml ps Name Command State Ports -------------------------------------------------------------------------------------------------zookeeper_1 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:2181-&gt;2181/tcp, 2888/tcp, 3888/tcpzookeeper_2 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:2182-&gt;2181/tcp, 2888/tcp, 3888/tcpzookeeper_3 /docker-entrypoint.sh zkSe ... Up 0.0.0.0:2183-&gt;2181/tcp, 2888/tcp, 3888/tcp 查看zookeeper集群状态 zoo1 123456$ docker exec -it zookeeper_1 /bin/sh/zookeeper-3.4.11 # zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /conf/zoo.cfgMode: follower // 这是个follower zoo2 123456$ docker exec -it zookeeper_2 /bin/sh/zookeeper-3.4.11 # zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /conf/zoo.cfgMode: leader // 这是个leader zoo3 123456$ docker exec -it zookeeper_3 /bin/sh/zookeeper-3.4.11 # zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /conf/zoo.cfgMode: follower // 这也是个follower哦 三个zookeeper服务都正常启动了","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"},{"name":"Docker","slug":"Server/Docker","permalink":"https://mrjerryli.github.io/categories/Server/Docker/"}],"tags":[{"name":"centos","slug":"centos","permalink":"https://mrjerryli.github.io/tags/centos/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"},{"name":"Docker","slug":"Server/Docker","permalink":"https://mrjerryli.github.io/categories/Server/Docker/"}]},{"title":"docker-compose在线升级","slug":"docker-compose在线升级","date":"2020-05-17T04:11:25.000Z","updated":"2020-05-17T04:20:31.071Z","comments":true,"path":"post/b82f2049.html","link":"","permalink":"https://mrjerryli.github.io/post/b82f2049.html","excerpt":"","text":"docker-compose在线升级这个比较简单，就三行命令。 获取最新版本 1curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose 注意： 其中1.23.2是可以更换为最新的版本号 {% image 20200517docker-compose在线升级/WX20200517-121523@2x.png '获取最新版本' '' %} 授权 1chmod +x /usr/local/bin/docker-compose 授权 查看版本 1docker-compose --version 获取最新版本 这样就在线升级完成了，比较简单。","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"},{"name":"Docker","slug":"Server/Docker","permalink":"https://mrjerryli.github.io/categories/Server/Docker/"}],"tags":[{"name":"centos","slug":"centos","permalink":"https://mrjerryli.github.io/tags/centos/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"},{"name":"Docker","slug":"Server/Docker","permalink":"https://mrjerryli.github.io/categories/Server/Docker/"}]},{"title":"阿里云配置Let’s Encrypt证书","slug":"阿里云配置Let’sEncrypt证书","date":"2020-05-16T20:06:25.000Z","updated":"2020-05-16T21:04:23.861Z","comments":true,"path":"post/7bd4d456.html","link":"","permalink":"https://mrjerryli.github.io/post/7bd4d456.html","excerpt":"","text":"Let’s Encrypt证书申请、阿里云配置Let’s Encrypt证书一、证书申请申请入口：https://letsencrypt.osfipin.com/申请成功，如下： 证书 点击查查，可进行证书的下载 证书下载 解压后得到六个文件 证书解压 压缩包文件说明 private.pem 密钥，可更改后缀为key fullchain.crt 完整证书，可更改后缀为pem certificate.pfx IIS和Tomcat使用，秘钥在detail.txt中 certificate.crt 域名证书，一般不用 二、证书安装登录阿里云控制台，打开SSL证书列表 ssl列表 点击上传证书，将证书文件及私钥进行上传 证书上传 证书上传 上传完成后，结果如下图，在已签发中可查询的到 证书已签发 阿里云ECS服务器打开443安全组访问规则 安全访问规则 SSH进入centos服务器，配置NGINX关于 HTTPS 配置，脚本如下： 在NGINX 配置目录conf下创建文件夹cert 1mkdir cert 上传申请免费域名的证书及秘钥文件，即fullchain.crt 与 cert/private.pem 编辑nginx.conf文件 1234567891011121314151617181920# HTTPS server server &#123; listen 443 ssl http2; server_name 你的域名; ssl_certificate cert/fullchain.crt; ssl_certificate_key cert/private.pem; ssl_protocols TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; location / &#123; root html; index index.html index.htm; &#125; &#125; 检查NGINX配置文件是否编辑正确，重启NGINX 1234567▶ ./nginx -t nginx: the configuration file /opt/nginx/conf/nginx.conf syntax is oknginx: configuration file /opt/nginx/conf/nginx.conf test is successful▶ ./nginx -s reload centos开放443端口，重启防火墙12345▶ firewall-cmd --add-port=443/tcp --permanent▶ firewall-cmd --reload 到这，安心的放你的HTTPS的网站把，已经大功告成。 注意： 在NGINX配置https时，可能会报错找不到 ssl,原因是在安装NGINX时，没有将ssl模块进行安装，该模块依赖于 openssl openssl-devel，安装脚本如下,随后从新编译安装NGINX就可以了。 yum -y install openssl openssl-devel 缺点就是证书只能维持三个月，不是长久，解决方案为使用脚本定时刷新更新证书，但是阿里云还是需要手工进行配置。","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}],"tags":[{"name":"centos","slug":"centos","permalink":"https://mrjerryli.github.io/tags/centos/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}]},{"title":"MAC使用homeBrew安装Redis","slug":"MAC使用homeBrew安装Redis","date":"2019-10-21T16:55:09.000Z","updated":"2019-10-21T16:58:13.026Z","comments":true,"path":"post/63201c18.html","link":"","permalink":"https://mrjerryli.github.io/post/63201c18.html","excerpt":"","text":"MAC使用homeBrew安装Redis一般买了macbook第一件事就是把homeBrew和iterm2装上,它们俩的组合可以实现绝大多数的软件安装。网上有很多教程安装它们的，关于homeBrew的操作命令如下: 1234567brew search ** //查找某个软件包brew list //列出已经安装的软件的包brew install ** //安装某个软件包,默认安装的是稳定版本brew uninstall **//卸载某个软件的包brew upgrade ** //更新某个软件包brew info ** //查看指定软件包的说明brew cache clean //清理缓存 现在我们尝试在mac上通过homeBrew安装redis 1234567 brew search redis //出现如下 ==&gt; Searching local taps...hiredis redis redis-leveldb redis@2.8 redis@3.2==&gt; Searching taps on GitHub...homebrew/cask/redis-app==&gt; Searching blacklisted, migrated and deleted formulae... 安装redis3.2版本 1brew install redis@3.2 安装完成提示如下 12345678910111213//把redis的环境变量配置到.zshrc中If you need to have redis@3.2 first in your PATH run: echo 'export PATH=\"/usr/local/opt/redis@3.2/bin:$PATH\"' &gt;&gt; ~/.zshrcTo have launchd start redis@3.2 now and restart at login://使用launchctl brew启动 brew services start redis@3.2 ### 使用配置文件启动Or, if you don't want/need a background service you can just run: redis-server /usr/local/etc/redis.conf==&gt; Summary🍺 /usr/local/Cellar/redis@3.2/3.2.12: 13 files, 1.7MB /usr/local/etc 下修改redis.config找到daemonize no改成yes 以守护进程的方式启动配置环境变量echo ‘export PATH=”/usr/local/opt/redis@3.2/bin:$PATH”‘ &gt;&gt; ~/.zshrc 启动:brew services start redis@3.2或者使用redis-server /usr/local/etc/redis.conf启动查看进程ps axu | grep redis 12isole 84721 0.0 0.0 4267768 900 s000 S+ 10:08上午 0:00.00 grep redisisole 84501 0.0 0.0 4309180 1568 ?? Ss 10:06上午 0:00.07 redis-server 127.0.0.1:6379 连接客户端:redis-cli -h 127.0.0.1 -p 6379如下:127.0.0.1:6379&gt; get(“123”) 关闭redis-cli shutdown杀死sudo pkill redis-server 关于redis.conf配置文件的说明 redis默认是前台启动,不是以守护进程的方式进行:daemonize no,把这里修改成yes,就可以让redis以守护进程的方式启动。 当redis使用守护进程方式运行,会默认把pid写入/var/run/reids.pid文件中,可以通过pidfile /var/run/redis.pid进行指定。 端口号指定,这个不用多说了,默认是6379,可以根据需要自己修改。 客户端如果一直连接着不释放的话会自动关闭连接,这是通过timeout 100来设定的,如果设置为0表示不会自动关闭。 设置redis数据库的数量, databases 16 默认是16 既然redis是一个数据库,就代表如果需要连接的话可能需要配置一个密码,默认是不需要密码的。requirepass foobared,客户端在链接时需要通过AUTH命令提供密码。等等还有很多，这里仅仅介绍了一点点的命令，直接修改redis.conf是一个方法,但是还有另外一个方法就是通过命令行去修改。redis 127.0.0.1:6379&gt; CONFIG GET CONFIG_SETTING_NAME 查看某个配置比如 : 我们要查看daemonize 直接输入redis 127.0.0.1:6379&gt; CONFIG GET daemonize修改的话直接:redis 127.0.0.1:6379&gt;CONFIG SET daemonize yes这样就可以了。","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://mrjerryli.github.io/tags/Mac/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}]},{"title":"springboot-thymeleaf动态模板生成","slug":"springboot-thymeleaf动态模板生成","date":"2019-10-12T12:13:18.000Z","updated":"2019-10-21T16:56:48.420Z","comments":true,"path":"post/457323eb.html","link":"","permalink":"https://mrjerryli.github.io/post/457323eb.html","excerpt":"","text":"springboot-thymeleaf动态模板生成 thymeleaf动态模板，案例 123456789101112131415Map data = new HashMap(); data.put(\"name\", \"jerry\"); data.put(\"email\", email); data.put(\"url\", \"https://www.lijiating.online/\"); data.put(\"secondairePath\", \"myblog\"); data.put(\"requestId\", \"\");SpringTemplateEngine springTemplateEngine = new SpringTemplateEngine();StringTemplateResolver stringTemplateResolver = new StringTemplateResolver();stringTemplateResolver.setCacheable(true);stringTemplateResolver.setTemplateMode(TemplateMode.HTML); springTemplateEngine.setTemplateResolver(stringTemplateResolver)Context context = new Context();context.setVariables(data);String content = \"&lt;span th:utext=\\\"$&#123;code&#125;\\\"&gt;&lt;/span&gt;\";String result = springTemplateEngine.process(content, context); pom文件引入： 12345&lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf&lt;/artifactId&gt; &lt;version&gt;3.0.9.RELEASE&lt;/version&gt;&lt;/dependency&gt; springboot可直接引入： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt;&lt;/dependency&gt;","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"https://mrjerryli.github.io/tags/springboot/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}]},{"title":"mac杀死端口","slug":"mac杀死端口","date":"2019-09-19T11:33:06.000Z","updated":"2019-09-19T11:42:16.709Z","comments":true,"path":"post/87630688.html","link":"","permalink":"https://mrjerryli.github.io/post/87630688.html","excerpt":"","text":"Mac 查看端口占用及杀死进程 查看端口占用情况 12345# jerry in ~ [14:04:53] C:1$ lsof -i :18088COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 27191 lijiating 287u IPv6 0xcebad4dccbf6e8d1 0t0 TCP *:18088 (LISTEN) 杀死占用端口的进程 12# jerry in ~ [14:05:08]$ kill -9 27191","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://mrjerryli.github.io/tags/linux/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}]},{"title":"ZooKeeper安装及配置 for Mac","slug":"zookeeper","date":"2019-09-10T13:45:35.000Z","updated":"2019-09-10T14:21:30.996Z","comments":true,"path":"post/3ad834c9.html","link":"","permalink":"https://mrjerryli.github.io/post/3ad834c9.html","excerpt":"","text":"ZooKeeper安装及配置 for Mac 安装 查看brew info zookeeper 1234$ brew info zookeeperzookeeper: stable 3.4.12 (bottled), HEADCentralized server for distributed coordination of serviceshttps://zookeeper.apache.org/ 安装brew install zookeeper 123456789 $ brew install zookeeper ==&gt; Downloading https://homebrew.bintray.com/bottles/zookeeper-3.4.12.high_sierr###### ################################################################## 100.0%==&gt; Pouring zookeeper-3.4.12.high_sierra.bottle.tar.gz==&gt; CaveatsTo have launchd start zookeeper now and restart at login: brew services start zookeeperOr, if you don't want/need a background service you can just run: zkServer start 安装后，在/usr/local/etc/zookeeper/ 中可查看配置 1$ ls /usr/local/etc/zookeeper/ 安装后，在/usr/local/etc/zookeeper/ 已经有了缺省配置 1$ ls /usr/local/etc/zookeeper/ 查看配置文件 1$ less -N /usr/local/etc/zookeeper/zoo.cfg 配置文件如下 12345678910111213141516171819202122232425262728# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/usr/local/var/run/zookeeper/data# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to \"0\" to disable auto purge feature#autopurge.purgeInterval=1 启动服务 执行命令zkServer 1234567891011121314$ zkServerZooKeeper JMX enabled by defaultUsing config: /usr/local/etc/zookeeper/zoo.cfgUsage: ./zkServer.sh &#123;start|start-foreground|stop|restart|status|upgrade|print-cmd&#125;$ zkServer statusZooKeeper JMX enabled by defaultUsing config: /usr/local/etc/zookeeper/zoo.cfgError contacting service. It is probably not running.$ zkServer startZooKeeper JMX enabled by defaultUsing config: /usr/local/etc/zookeeper/zoo.cfgStarting zookeeper ... STARTED 查看zookeeper运行及状态 安装后，可以看到zookeeper提供了zkCli等工具进行. 1234567891011121314151617$ zkServer statusZooKeeper JMX enabled by defaultUsing config: /usr/local/etc/zookeeper/zoo.cfgError contacting service. It is probably not running.192:~ maerfeifei$ zkServer startZooKeeper JMX enabled by defaultUsing config: /usr/local/etc/zookeeper/zoo.cfgStarting zookeeper ... STARTED192:~ maerfeifei$ zkCliConnecting to localhost:2181Welcome to ZooKeeper!JLine support is enabledWATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0]","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://mrjerryli.github.io/tags/zookeeper/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}]},{"title":"yarn切换镜像源","slug":"yarn","date":"2019-08-28T08:23:25.000Z","updated":"2019-08-28T08:32:36.881Z","comments":true,"path":"post/c965c636.html","link":"","permalink":"https://mrjerryli.github.io/post/c965c636.html","excerpt":"","text":"1、查看一下当前源 yarn config get registry 2、切换为淘宝源 yarn config set registry https://registry.npm.taobao.org 3、或者切换为自带的 yarn config set registry https://registry.yarnpkg.com","categories":[{"name":"yarn","slug":"yarn","permalink":"https://mrjerryli.github.io/categories/yarn/"}],"tags":[{"name":"web","slug":"web","permalink":"https://mrjerryli.github.io/tags/web/"}],"keywords":[{"name":"yarn","slug":"yarn","permalink":"https://mrjerryli.github.io/categories/yarn/"}]},{"title":"centos","slug":"centos","date":"2019-08-04T23:32:01.000Z","updated":"2019-08-11T03:39:26.557Z","comments":true,"path":"post/cd183338.html","link":"","permalink":"https://mrjerryli.github.io/post/cd183338.html","excerpt":"","text":"CentOS 7 开放3306端口访问 CentOS 7.0默认使用的是firewall作为防火墙，这里改为iptables防火墙。1、关闭firewall：systemctl stop firewalld.servicesystemctl disable firewalld.servicesystemctl mask firewalld.service 2、安装iptables防火墙yum install iptables-services -y 3.启动设置防火墙 systemctl enable iptablessystemctl start iptables4.查看防火墙状态 systemctl status iptables 5编辑防火墙，增加端口vi /etc/sysconfig/iptables #编辑防火墙配置文件-A INPUT -m state –state NEW -m tcp -p tcp –dport 22 -j ACCEPT-A INPUT -m state –state NEW -m tcp -p tcp –dport 80 -j ACCEPT-A INPUT -m state –state NEW -m tcp -p tcp –dport 3306 -j ACCEPT 或者 -A INPUT -p tcp -m state –state NEW -m tcp –dport 22 -j ACCEPT-A INPUT -p tcp -m state –state NEW -m tcp –dport 80 -j ACCEPT-A INPUT -p tcp -m state –state NEW -m tcp –dport 3306 -j ACCEPT :wq! #保存退出 3.重启配置，重启系统systemctl restart iptables.service #重启防火墙使配置生效systemctl enable iptables.service #设置防火墙开机启动","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}],"tags":[{"name":"centos","slug":"centos","permalink":"https://mrjerryli.github.io/tags/centos/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}]},{"title":"protobuf","slug":"protobuf","date":"2019-08-03T20:53:47.000Z","updated":"2019-08-11T01:39:47.195Z","comments":true,"path":"post/99fac5be.html","link":"","permalink":"https://mrjerryli.github.io/post/99fac5be.html","excerpt":"","text":"protobuf 什么是protobuf官方解释：Protocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. 意思是: Protocol buffers是Google的语言中立，平台中立，可扩展的机制，用于序列化结构化数据 - 类似XML，但更小，更快，更简单。 使用 Protobuf 你可以编写一次结构化数据一次，然后可以使用各种语言工具来生成对应语言的源代码然后简单的读取或者操作数据。 抓重点，语言中立， 工具，源代码 使用从上面的介绍中我们可以看到在使用 Protobuf 的使用需要有一个对应语言的工具，通过工具生成对应的源代码，然后在操作相应结构的数据。下面我们依次看下具体是如何使用的。 首先我们这里采用的是 java 语言，所以要先去下载 Java 对应的工具通过链接https://github.com/protocolbuffers/protobuf/releases/tag/v3.8.0下载 Java 的工具，macOS 可以直接使用brew install protobuf 编写 .proto 文件，在使用 Protobuf 前，我们要编写结构化的数据格式，例如我们这里编写 com-test-model-User.proto 文件 1234567891011121314//指定版本syntax = &quot;proto2&quot;;//定义包名package com.test.model;//定义结构数据message User &#123; //必选字段 第1个属性 required string name = 1; //必选字段 第2个属性 required int32 age = 2; //可选字段 第3个属性 optional string comment = 3;&#125; 编写完了采用命令protoc --java_out=. ./com-test-model-User.proto，就会在当前路径下生成相应的代码结构。 使用案例1234567891011121314151617181920212223242526272829303132public class Test &#123; public static void main(String[] args) throws IOException &#123; UserOuterClass.User.Builder userBuilder = UserOuterClass.User.newBuilder(); userBuilder.setName(\"子悠\"); userBuilder.setAge(18);// userBuilder.setComment(\"this is comment\"); System.out.println(\"\\n**********************序列化*****************************\"); byte[] bytes = userBuilder.build().toByteArray(); System.out.println(\"bytes length is \" + bytes.length); for (int i = 0; i &lt; bytes.length; i++) &#123; System.out.print(bytes[i] + \" \"); &#125; System.out.println(\"\\n**********************反序列化*****************************\"); UserOuterClass.User user = UserOuterClass.User.parseFrom(bytes); System.out.println(user.getName()); &#125;&#125;//运行结果**********************序列化*****************************bytes length is 1010 6 -27 -83 -112 -26 -126 -96 16 18 **********************反序列化*****************************子悠Process finished with exit code 0 结果分析 可能到现在大家还没有发现什么优秀的地方，那么让我解释下。从运行结果来看，序列化后的是一串数字。很简短的一串数字。我们可以想一下如果这里用的 JSON 格式的序列化的话那么结果应该是{&quot;name&quot;: &quot;子悠&quot;, &quot;age&quot;: 18}，如果是 xml 的话，那就会更长，从这里我们就可以看出 protobuf 的序列化的效果是多么的强大，效率是多么的高。我们知道在网络传输的过程中，压缩效率越高传输效率就越高。 1234&lt;user&gt; &lt;name&gt;子悠&lt;/name&gt; &lt;age&gt;18&lt;/age&gt;&lt;/user&gt; protobuf 优缺点 更加简单。 数据体积小 3- 10 倍。 更快的反序列化速度，提高 20 - 100 倍。 可以自动化生成更易于编码方式使用的数据访问类。 总结 简单给大家介绍了 protobuf 的简单使用，更多的详细使用，以及底层压缩原理，感兴趣的朋友可以自己的研究一下。 另外说个题外话 protocol buffers 诞生之初是为了解决服务器端新旧协议(高低版本)兼容性问题，名字也很体贴，“协议缓冲区”。只不过后期慢慢发展成用于传输数据。 有时候就是这样，一个项目或者软件的最终形态并不是当时定义的模样，随着时间的推移产品的方向以及定位都会发生翻天覆地的变化。 附属GitHub项目demo地址","categories":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}],"tags":[{"name":"netty","slug":"netty","permalink":"https://mrjerryli.github.io/tags/netty/"},{"name":"protobuf","slug":"protobuf","permalink":"https://mrjerryli.github.io/tags/protobuf/"}],"keywords":[{"name":"Server","slug":"Server","permalink":"https://mrjerryli.github.io/categories/Server/"}]}]}